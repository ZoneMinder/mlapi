[general]
# This is an optional file
# If specified, you can specify tokens with secret values in that file
# and onlt refer to the tokens in your main config file

#secrets=./secrets.ini
secrets=/etc/zm/secrets.ini


# port that mlapi will listen on. Default 5000
port=5000

# Maximum # of processes that will be forked
# to handle requests. Note that each process will
# have its own copy of the model, so memory can 
# build up very quickly
# This number also dictates how many requests will be executed in parallel
# The rest will be queued

# default: flask
wsgi_server=bjoern 


# If you are using bjoern, processes is always 1 
# For now, keep this to 1 if you are on a GPU
processes=1

# the secret key that will be used to sign
# JWT tokens. Make sure you change the value
# in your secrets.ini
mlapi_secret_key=!MLAPI_SECRET_KEY

# folder where images will be uploaded
# default ./images
images_path=./images

# folder where the user DB will be stored
db_path=./db

# If specified, will limit detected object size to this amount of
# the total image size passed. Can help avoiding weird detections
# You can specify as % or px. Default is px
# Remember the image is resized to 416x416 internally. better
# to keep in %
max_detection_size=100%

# You can now limit the # of detection process
# per target processor. If not specified, default is 1
# Other detection processes will wait to acquire lock

cpu_max_processes=3
tpu_max_processes=1
gpu_max_processes=1

# NEW: Time to wait in seconds per processor to be free, before
# erroring out. Default is 120 (2 mins)
cpu_max_lock_wait=120
tpu_max_lock_wait=120
gpu_max_lock_wait=120


[ml]

# If enabled (default = no), we will use ml_sequence and ignore everything
# in [object] [face] and [alpr]
#
# use_sequence = yes

# You can use this structure to create a chain of detection algorithms
# See https://pyzm.readthedocs.io/en/latest/source/pyzm.html#detectsequence for options (look at options field)
# very important - please make sure that the curly braces need to be indented to be inside ml_sequence
# tag, or parsing will fail
# NOTE: If you specify this attribute, all settings in [object], [face] and [alpr] will be ignored

use_sequence = yes

stream_sequence = {
        'frame_strategy': 'first',
        'frame_set': 'snapshot,alarm'

    }

ml_sequence= {
                'general': {
                        'model_sequence': 'object,face,alpr',

                },
                'object': {
                        'general':{
                                'pattern':'(person)',
                                'same_model_sequence_strategy': 'first' # also 'most', 'most_unique's
                        },
                        'sequence': [{
                                #First run on TPU with higher confidence
                                'object_weights':'/var/lib/zmeventnotification/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite',
                                'object_labels': '/var/lib/zmeventnotification/models/coral_edgetpu/coco_indexed.names',
                                'object_min_confidence': 0.6,
                                'object_framework':'coral_edgetpu'
                        },
                        {
                                # YoloV4 on GPU if TPU fails (because sequence strategy is 'first')
                                'object_config':'/var/lib/zmeventnotification/models/yolov4/yolov4.cfg',
                                'object_weights':'/var/lib/zmeventnotification/models/yolov4/yolov4.weights',
                                'object_labels': '/var/lib/zmeventnotification/models/yolov4/coco.names',
                                'object_min_confidence': 0.3,
                                'object_framework':'opencv',
                                'object_processor': 'gpu'
                        }]
                },
                'face': {
                        'general':{
                                'pattern': '.*',
                                'same_model_sequence_strategy': 'first'
                        },
                        'sequence': [{
                                'face_detection_framework': 'dlib',
                                'known_images_path': '/var/lib/zmeventnotification/known_faces',
                                'unknown_images_path': '/var/lib/zmeventnotification/unknown_faces',

                                'face_model': 'cnn',
                                'face_train_model': 'cnn',
                                'face_recog_dist_threshold': 0.6,
                                'face_num_jitters': 1,
                                'face_upsample_times':1
                        }]
                },

                'alpr': {
                        'general':{
                                'same_model_sequence_strategy': 'first',
                                'pre_existing_labels':['car', 'motorbike', 'bus', 'truck', 'boat'],

                        },
                        'sequence': [{
                                'alpr_api_type': 'cloud',
                                'alpr_service': 'plate_recognizer',
                                'alpr_key': '!PLATEREC_ALPR_KEY',
                                'platrec_stats': 'no',
                                'platerec_min_dscore': 0.1,
                                'platerec_min_score': 0.2,
                        }]
                }
        }


[object]

# for Yolov3
object_framework=opencv
object_processor=cpu
object_config=./models/yolov3/yolov3.cfg
object_weights=./models/yolov3/yolov3.weights
object_labels=./models/yolov3/coco.names

# for Tiny Yolov3
#object_framework=opencv
#object_processor=cpu
#object_config=./models/tinyyolov3/yolov3-tiny.cfg
#object_weights=./models/tinyyolov3/yolov3-tiny.weights
#object_labels=./models/tinyyolov3/coco.names

# for Yolov4
#object_framework=opencv
#object_processor=cpu
#object_config=./models/yolov4/yolov4.cfg
#object_weights=./models/yolov4/yolov4.weights
#object_labels=./models/yolov4/coco.names

# for Tiny Yolov4
#object_framework=opencv
#object_processor=cpu
#object_config=./models/tinyyolov4/yolov4-tiny.cfg
#object_weights=./models/tinyyolov4/yolov4-tiny.weights
#object_labels=./models/tinyyolov4/coco.names

# for Google Coral Edge TPU
#object_framework=coral_edgetpu
#object_processor=tpu
#object_weights=./models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite
#object_labels=./models/coral_edgetpu/coco_indexed.names


[face]
face_detection_framework=dlib
face_recognition_framework=dlib
face_num_jitters=0
face_upsample_times=1
face_model=cnn
face_train_model=hog
face_recog_dist_threshold=0.6
face_recog_knn_algo=ball_tree

known_images_path=./known_faces
unknown_images_path=./unknown_faces

unknown_face_name=unknown face
save_unknown_faces=yes
save_unknown_faces_leeway_pixels=50

[alpr]

alpr_use_after_detection_only=yes
alpr_api_type=cloud

# -----| If you are using plate recognizer | ------
alpr_service=plate_recognizer
alpr_key=!PLATEREC_ALPR_KEY
platerec_stats=yes
#platerec_regions=['us','cn','kr']
platerec_min_dscore=0.1
platerec_min_score=0.2

# ----| If you are using openALPR |-----
#alpr_service=open_alpr
#alpr_key=!OPENALPR_ALPR_KEY
#openalpr_recognize_vehicle=1
#openalpr_country=us
#openalpr_state=ca
# openalpr returns percents, but we convert to between 0 and 1
#openalpr_min_confidence=0.3

# ----| If you are using openALPR command line |-----
openalpr_cmdline_binary=alpr
openalpr_cmdline_params=-j -d
openalpr_cmdline_min_confidence=0.3
